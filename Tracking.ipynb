{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VgckmsXGXHCe","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5LPTZW55-zl","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","\n","import csv\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","from skimage import io, transform\n","import torch.nn.functional as F\n","import pandas as pd\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import SequentialSampler,Dataset,DataLoader\n","from torchvision import transforms, utils\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import time\n","import os\n","import copy\n","import cv2\n","import glob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3lLmJ2KCbgbv","colab_type":"code","colab":{}},"source":["def readCsv(header,data,name):\n","  with open(name, 'wt') as f:\n","    csvPrinter = csv.writer(f, quoting=csv.QUOTE_ALL)\n","    csvPrinter.writerow(header)\n","    csvPrinter.writerows(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aL-ZZXYMCSKt","colab_type":"code","colab":{}},"source":["\n","\n","\n","Annotations = sorted([k for k in os.listdir('/content/drive/My Drive/dataset/annotations/') \n","                   if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/annotations/', k))])\n","\n","DataAnn = []\n","for ann in Annotations:\n","  directory = '/content/drive/My Drive/dataset/annotations/' + ann\n","  file = open(directory, \"r\").read().split(\"\\n\")\n","  for line in range(len(file)-1):\n","      values = file[line].split()\n","      values2 = file[line+1].split()[1:]\n","      data = [ann[:-4]] + values + values2\n","      DataAnn.append(data)\n","      if line == len(file)-3:\n","        break\n","    \n","trains = os.listdir ('/content/drive/My Drive/dataset/videos/val/')\n","\n","DataTrain = []\n","for i in trains:\n","  file = sorted([j for j in os.listdir('/content/drive/My Drive/dataset/videos/val/' + i) \n","                 if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/videos/val/' + i, j))])\n","  t0 = \"\"\n","  t1 = \"\"\n","  for frame in file:\n","    if t0!= \"\":\n","      if t1 != \"\":        \n","        t0 = t1\n","        t1 = frame\n","      else:\n","        t1 = frame\n","      data = [i] +[int(t0[:-4])] + [t0] + [t1] \n","      DataTrain.append(data)\n","    else:\n","      t0 = frame\n","\n","header = ['video', 'frame_id', 'x1', 'y1', 'x2', 'y2','x11','y11','x22','y22']\n","readCsv(header,DataAnn,\"Annos.csv\")\n","header = ['video','frame_id', 'frame1','frame2']\n","readCsv(header,DataTrain,\"Test.csv\")\n","annos = pd.read_csv(\"Annos.csv\")\n","tests = pd.read_csv(\"Test.csv\")\n","merged = pd.merge(annos,tests)\n","merged.head()\n","merged.to_csv(\"/content/drive/My Drive/mergedValidation.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OESWL43ICSHh","colab_type":"code","colab":{}},"source":["Annotations = sorted([k for k in os.listdir('/content/drive/My Drive/dataset/annotations/') \n","                   if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/annotations/', k))])\n","DataAnn = []\n","for ann in Annotations:\n","  directory = '/content/drive/My Drive/dataset/annotations/' + ann\n","  file = open(directory, \"r\").read().split(\"\\n\")\n","  for line in range(len(file)-1):\n","      values = file[line].split()\n","      values2 = file[line+1].split()[1:]\n","      data = [ann[:-4]] + values + values2\n","      DataAnn.append(data)\n","      if line == len(file)-3:\n","        break\n","trains = os.listdir ('/content/drive/My Drive/dataset/videos/train/')\n","\n","DataTrain = []\n","for i in trains:\n","  file = sorted([j for j in os.listdir('/content/drive/My Drive/dataset/videos/train/' + i)\n","                 if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/videos/train/' + i, j))])\n","  t0 = \"\"\n","  t1 = \"\"\n","  for frame in file:\n","    if t0!= \"\":\n","      if t1 != \"\":        \n","        t0 = t1\n","        t1 = frame\n","      else:\n","        t1 = frame\n","      data = [i] +[int(t0[:-4])] + [t0] + [t1] \n","      DataTrain.append(data)\n","    else:\n","      t0 = frame\n","header = ['video', 'frame_id', 'x1', 'y1', 'x2', 'y2','x11','y11','x22','y22']\n","readCsv(header,DataAnn,\"Annos.csv\")\n","header = ['video','frame_id', 'frame1','frame2']\n","readCsv(header,DataTrain,\"Test.csv\")\n","annos = pd.read_csv(\"Annos.csv\")\n","tests = pd.read_csv(\"Test.csv\")\n","merged = pd.merge(annos,tests)\n","merged.head()\n","merged.to_csv(\"/content/drive/My Drive/mergedTrain.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibmhRXsA-fkv","colab_type":"code","colab":{}},"source":["Annotations = sorted([f for f in os.listdir('/content/drive/My Drive/dataset/annotations/') \n","                   if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/annotations/', f))])\n","\n","DataAnn = []\n","for ann in Annotations:\n","  file = open('/content/drive/My Drive/dataset/annotations/' + ann, \"r\").read().split(\"\\n\")\n","  for line in range(len(file)-1):\n","      values = file[line].split()\n","      data = [ann[:-4]] + values\n","      DataAnn.append(data)\n","    \n","trains = os.listdir ('/content/drive/My Drive/dataset/videos/test/')\n","\n","DataTrain = []\n","for i in trains:\n","  file = sorted([k for k in os.listdir('/content/drive/My Drive/dataset/videos/test/' + \n","                                      i) if os.path.isfile(os.path.join('/content/drive/My Drive/dataset/videos/test/' + i, k))])\n","  t0 = \"\"\n","  t1 = \"\"\n","  for frame in file:\n","    if t0!= \"\":\n","      if t1 != \"\":        \n","        t0 = t1\n","        t1 = frame\n","      else:\n","        t1 = frame\n","      data = [i] +[int(t0[:-4])] + [t0] + [t1] \n","      DataTrain.append(data)\n","    else:\n","      t0 = frame\n","\n","header = ['video', 'frameId', 'x1', 'y1', 'x2', 'y2']\n","readCsv(header,DataAnn,\"Annos.csv\")\n","header = ['video','frame_id', 'frame1','frame2']\n","readCsv(header,DataTrain,\"Test.csv\")\n","\n","annos = pd.read_csv(\"Annos3.csv\")\n","tests = pd.read_csv(\"Test.csv\")\n","merged = pd.merge(annos,tests)\n","merged.head()\n","merged.to_csv(\"/content/drive/My Drive/mergedTest.csv\", index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m89k_UDnSDBU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hq20tbHHVIIf","colab_type":"code","colab":{}},"source":["def normalizeVector(list):\n","    vector = []\n","    for x in list:\n","        normalized = (x-min(list))/(max(list)-min(list))\n","        vector.append(normalized)\n","    return vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXB6yPnSUB0s","colab_type":"code","colab":{}},"source":["class TrainDataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir \n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        video_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 0])\n","        frame_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 10])\n","        image = io.imread(frame_name)\n","        frame2_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 11])\n","        image2 = io.imread(frame2_name)\n","        landmarks = self.landmarks_frame.iloc[idx, 2:6].values\n","        \n","        landmarks2 = self.landmarks_frame.iloc[idx, 6:10].values\n","        \n","        img = np.array(image)\n","        img2 = np.array(image2)\n","        \n","        \n","        coordinates = []\n","        coordinates2 = []\n","        x1 = landmarks[0]\n","        y1 = landmarks[1]\n","        x2 = landmarks[2]\n","        y2 = landmarks[3]\n","        \n","        x11 = landmarks2[0]\n","        y11 = landmarks2[1]\n","        x22 = landmarks2[2]\n","        y22 = landmarks2[3]\n","        box_h = y2 - y1\n","        box_w = x2 - x1\n","        bbox = patches.Rectangle((x11, y11), x22-x11, y22-y11,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        if int(y1-(box_h/2))<0 and int(x1-(box_w/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = 0\n","        elif int(y1-(box_h/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedY1 = 0\n","          croppedX1 = x1-box_w/2\n","        elif int(x1-(box_w/2))<0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = y1-box_h/2\n","        elif int(y1-(box_h/2))>=0 and int(x1-(box_w/2))>=0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = x1-box_w/2\n","          croppedY1 = y1-box_h/2\n","        \n","  \n","        croppedX2 = x1+box_w/2\n","        croppedY2 = y1+box_h/2\n","      \n","        nx1 = x1 - croppedX1\n","        nx2 = x2 - croppedX1\n","        ny1 = y1 - croppedY1\n","        ny2 = y2 - croppedY1\n","      \n","        nx1 = nx1 * (224/len(croppedImage1[0]))\n","        nx2 = nx2 * (224/len(croppedImage1[0]))\n","        ny1 = ny1 * (224/len(croppedImage1))\n","        ny2 = ny2 * (224/len(croppedImage1))\n","      \n","        box_w1 = nx2-nx1\n","        box_h1 = ny2-ny1\n","      \n","        nx11 = x11 - croppedX1\n","        nx22 = x22 - croppedX1\n","        ny11 = y11 - croppedY1\n","        ny22 = y22 - croppedY1\n","        \n","        nx11 = nx11 * (224/len(croppedImage2[0]))\n","        nx22 = nx22 * (224/len(croppedImage2[0]))\n","        ny11 = ny11 * (224/len(croppedImage2))\n","        ny22 = ny22 * (224/len(croppedImage2))\n","        \n","        \n","        \n","        croppedX2 = croppedX2 - croppedX1\n","        croppedY2 = croppedY2 - croppedY1\n","        croppedX1 = 0\n","        croppedY1 = 0\n","  \n","        box_w2 = nx22-nx11\n","        box_h2 = ny22-ny11\n","      \n","      \n","        \n","        coordinates.append(nx1)\n","        coordinates.append(ny1)\n","        coordinates.append(nx2)\n","        coordinates.append(ny2)\n","        \n","        coordinates2.append(nx11)\n","        coordinates2.append(ny11)\n","        coordinates2.append(nx22)\n","        coordinates2.append(ny22)\n","        bbox2 = patches.Rectangle((nx11, ny11), box_w2, box_h2,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        croppedImage1 = self.transform(croppedImage1)\n","        croppedImage2 = self.transform(croppedImage2)\n","        x = torchvision.utils.make_grid(croppedImage1, nrow=5).permute(1, 2, 0)\n","        y = torchvision.utils.make_grid(croppedImage2, nrow=5).permute(1, 2, 0)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(x)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(y)\n","#         plt.figure()\n","#         fig, ax = plt.subplots(1)\n","#         ax.add_patch(bbox)\n","#         plt.imshow(img2)\n","#         plt.text(nx11, y11, s=str(nx11)+\",\"+str(ny11), \n","#                 color='white', verticalalignment='top',\n","#                 bbox={'color': \"green\", 'pad': 0})\n","#         plt.axis('off')\n","#         plt.show()\n","  \n","        \n","        coordinates = torch.from_numpy(np.array(coordinates))\n","        coordinates2 = torch.from_numpy(np.array(coordinates2))\n","      \n","        sample1 = (x,y,coordinates,coordinates2)\n","        return sample1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPMtzerET3RV","colab_type":"code","colab":{}},"source":["def cropImage(image,x1,x2,y1,y2):\n","        box_h = y2 - y1\n","        box_w = x2 - x1\n","        if int(y1-(box_h/2))<0 and int(x1-(box_w/2))<0:\n","          croppedImage = image[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = 0\n","        elif int(y1-(box_h/2))<0:\n","          croppedImage = image[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedY1 = 0\n","          croppedX1 = x1-box_w/2\n","        elif int(x1-(box_w/2))<0:\n","          croppedImage = image[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = y1-box_h/2\n","        elif int(y1-(box_h/2))>=0 and int(x1-(box_w/2))>=0:\n","          croppedImage = image[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = x1-box_w/2\n","          croppedY1 = y1-box_h/2\n","          \n","        croppedX2 = x1+box_w/2\n","        croppedY2 = y1+box_h/2\n","      \n","        nx1 = x1 - croppedX1\n","        nx2 = x2 - croppedX1\n","        ny1 = y1 - croppedY1\n","        ny2 = y2 - croppedY1\n","        return croppedImage,nx1,nx2,ny1,ny2,croppedX1,croppedX2,croppedY1,croppedY2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEwCOG80GnvP","colab_type":"code","colab":{}},"source":["class ValidDataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir \n","        self.transform = transform\n","        \n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        video_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 0])\n","        frame_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 10])\n","        image = io.imread(frame_name)\n","        frame2_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 11])\n","        image2 = io.imread(frame2_name)\n","        landmarks = self.landmarks_frame.iloc[idx, 2:6].values\n","        \n","        landmarks2 = self.landmarks_frame.iloc[idx, 6:10].values\n","        \n","        img = np.array(image)\n","        img2 = np.array(image2)\n","        coordinates = []\n","        coordinates2 = []\n","        x1 = landmarks[0]\n","        y1 = landmarks[1]\n","        x2 = landmarks[2]\n","        y2 = landmarks[3]\n","        \n","        x11 = landmarks2[0]\n","        y11 = landmarks2[1]\n","        x22 = landmarks2[2]\n","        y22 = landmarks2[3]\n","        box_h = y2 - y1\n","        box_w = x2 - x1\n","        bbox = patches.Rectangle((x1, y1), box_w, box_h,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        if int(y1-(box_h/2))<0 and int(x1-(box_w/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = 0\n","        elif int(y1-(box_h/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedY1 = 0\n","          croppedX1 = x1-box_w/2\n","        elif int(x1-(box_w/2))<0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = y1-box_h/2\n","        elif int(y1-(box_h/2))>=0 and int(x1-(box_w/2))>=0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = x1-box_w/2\n","          croppedY1 = y1-box_h/2\n","        \n","  \n","        croppedX2 = x1+box_w/2\n","        croppedY2 = y1+box_h/2\n","      \n","        nx1 = x1 - croppedX1\n","        nx2 = x2 - croppedX1\n","        ny1 = y1 - croppedY1\n","        ny2 = y2 - croppedY1\n","      \n","        nx1 = nx1 * (224/len(croppedImage1[0]))\n","        nx2 = nx2 * (224/len(croppedImage1[0]))\n","        ny1 = ny1 * (224/len(croppedImage1))\n","        ny2 = ny2 * (224/len(croppedImage1))\n","      \n","        box_w1 = nx2-nx1\n","        box_h1 = ny2-ny1\n","      \n","        nx11 = x11 - croppedX1\n","        nx22 = x22 - croppedX1\n","        ny11 = y11 - croppedY1\n","        ny22 = y22 - croppedY1\n","        \n","        nx11 = nx11 * (224/len(croppedImage2[0]))\n","        nx22 = nx22 * (224/len(croppedImage2[0]))\n","        ny11 = ny11 * (224/len(croppedImage2))\n","        ny22 = ny22 * (224/len(croppedImage2))\n","        \n","        croppedX2 = croppedX2 - croppedX1\n","        croppedY2 = croppedY2 - croppedY1\n","        croppedX1 = 0\n","        croppedY1 = 0\n","  \n","        box_w2 = nx22-nx11\n","        box_h2 = ny22-ny11\n","      \n","        coordinates.append(nx1)\n","        coordinates.append(ny1)\n","        coordinates.append(nx2)\n","        coordinates.append(ny2)\n","        \n","        coordinates2.append(nx11)\n","        coordinates2.append(ny11)\n","        coordinates2.append(nx22)\n","        coordinates2.append(ny22)\n","        bbox2 = patches.Rectangle((nx11, ny11), box_w2, box_h2,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        croppedImage1 = self.transform(croppedImage1)\n","        croppedImage2 = self.transform(croppedImage2)\n","        x = torchvision.utils.make_grid(croppedImage1, nrow=5).permute(1, 2, 0)\n","        y = torchvision.utils.make_grid(croppedImage2, nrow=5).permute(1, 2, 0)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(x)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(y)\n","#         plt.figure()\n","#         fig, ax = plt.subplots(1)\n","#         ax.add_patch(bbox2)\n","#         plt.imshow(y)\n","#         plt.text(nx11, y11, s=str(nx11)+\",\"+str(ny11), \n","#                 color='white', verticalalignment='top',\n","#                 bbox={'color': \"green\", 'pad': 0})\n","#         plt.axis('off')\n","#         plt.show()\n","        \n","        coordinates = torch.from_numpy(np.array(coordinates))\n","        coordinates2 = torch.from_numpy(np.array(coordinates2))\n","      \n","        \n","        sample1 = (x,y,coordinates,coordinates2)\n","        return sample1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFjhJL1DnNzP","colab_type":"code","colab":{}},"source":["class TestDataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        self.landmarks_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir \n","        self.transform = transform\n","        \n","\n","    def __len__(self):\n","        return len(self.landmarks_frame)\n","\n","    def __getitem__(self, idx):\n","        video_name = os.path.join(self.root_dir,\n","                                self.landmarks_frame.iloc[idx, 0])\n","        frame_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 6])\n","        image = io.imread(frame_name)\n","        frame2_name = os.path.join(video_name,\n","                                self.landmarks_frame.iloc[idx, 7])\n","        image2 = io.imread(frame2_name)\n","        landmarks = self.landmarks_frame.iloc[idx, 2:6].values\n","        \n","        img = np.array(image)\n","        img2 = np.array(image2)\n","        coordinates = []\n","        coordinates2 = []\n","        x1 = landmarks[0]\n","        y1 = landmarks[1]\n","        x2 = landmarks[2]\n","        y2 = landmarks[3]\n","        \n","        box_h = y2 - y1\n","        box_w = x2 - x1\n","        bbox = patches.Rectangle((x1, y1), box_w, box_h,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        if int(y1-(box_h/2))<0 and int(x1-(box_w/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = 0\n","        elif int(y1-(box_h/2))<0:\n","          croppedImage1 = img[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[0:int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedY1 = 0\n","          croppedX1 = x1-box_w/2\n","        elif int(x1-(box_w/2))<0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),0:int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = 0\n","          croppedY1 = y1-box_h/2\n","        elif int(y1-(box_h/2))>=0 and int(x1-(box_w/2))>=0:\n","          croppedImage1 = img[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedImage2 = img2[int(y1-(box_h/2)):int(y1+box_h+(box_h/2)),int(x1-(box_w/2)):int(x1+box_w+(box_w/2)),:]\n","          croppedX1 = x1-box_w/2\n","          croppedY1 = y1-box_h/2\n","        \n","  \n","        croppedX2 = x1+box_w/2\n","        croppedY2 = y1+box_h/2\n","      \n","        nx1 = x1 - croppedX1\n","        nx2 = x2 - croppedX1\n","        ny1 = y1 - croppedY1\n","        ny2 = y2 - croppedY1\n","      \n","        nx1 = nx1 * (224/len(croppedImage1[0]))\n","        nx2 = nx2 * (224/len(croppedImage1[0]))\n","        ny1 = ny1 * (224/len(croppedImage1))\n","        ny2 = ny2 * (224/len(croppedImage1))\n","      \n","        box_w1 = nx2-nx1\n","        box_h1 = ny2-ny1\n","      \n","        \n","        \n","        croppedX2 = croppedX2 - croppedX1\n","        croppedY2 = croppedY2 - croppedY1\n","        croppedX1 = 0\n","        croppedY1 = 0\n","  \n","      \n","        coordinates.append(nx1)\n","        coordinates.append(ny1)\n","        coordinates.append(nx2)\n","        coordinates.append(ny2)\n","        \n","#         bbox2 = patches.Rectangle((nx11, ny11), box_w2, box_h2,\n","#              linewidth=2, facecolor='none',edgecolor = \"white\")\n","        \n","        croppedImage1 = self.transform(croppedImage1)\n","        croppedImage2 = self.transform(croppedImage2)\n","        x = torchvision.utils.make_grid(croppedImage1, nrow=5).permute(1, 2, 0)\n","        y = torchvision.utils.make_grid(croppedImage2, nrow=5).permute(1, 2, 0)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(x)\n","#         plt.figure()\n","#         plt.title(video_name)\n","#         plt.imshow(y)\n","#         plt.figure()\n","#         fig, ax = plt.subplots(1)\n","#         ax.add_patch(bbox2)\n","#         plt.imshow(y)\n","#         plt.text(nx11, y11, s=str(nx11)+\",\"+str(ny11), \n","#                 color='white', verticalalignment='top',\n","#                 bbox={'color': \"green\", 'pad': 0})\n","#         plt.axis('off')\n","#         plt.show()\n","        \n","      \n","        coordinates = torch.from_numpy(np.array(coordinates))\n","      \n","        \n","        sample1 = (x,y,coordinates,torch.from_numpy(img2))\n","        return sample1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4xztgCYjYDK","colab_type":"code","colab":{}},"source":["train_dataset = TrainDataset(csv_file='/content/drive/My Drive/mergedTrain.csv',\n","                                    root_dir='drive/My Drive/dataset/videos/train',transform = transforms.Compose([transforms.ToPILImage(),\n","                                                                                                                  transforms.Resize((224,224)),\n","                                                                                                                  transforms.ToTensor()]))\n","\n","\n","\n","\n","\n","\n","valid_dataset = ValidDataset(csv_file = '/content/drive/My Drive/mergedValidation.csv',root_dir='drive/My Drive/dataset/videos/val',transform = transforms.Compose([transforms.ToPILImage(),\n","                                                                                                                          transforms.Resize((224,224)),\n","                                                                                                                          transforms.ToTensor()]))\n","\n","\n","\n","\n","test_dataset = TestDataset(csv_file = '/content/drive/My Drive/mergedTest.csv',root_dir='drive/My Drive/dataset/videos/test',transform = transforms.Compose([transforms.ToPILImage(),\n","                                                                                                                             transforms.Resize((224,224)),\n","                                                                                                                             transforms.ToTensor()]))\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","validLoader = torch.utils.data.DataLoader(valid_dataset, batch_size=1,\n","                         shuffle=True, num_workers=4)\n","\n","\n","trainLoader = torch.utils.data.DataLoader(train_dataset, batch_size=1,\n","                         shuffle=True, num_workers=4)\n","\n","\n","testLoader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n","                         shuffle=False, num_workers=4)\n"," \n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3A2wwZaKzzyR","colab_type":"code","colab":{}},"source":["class ConvNeuralNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNeuralNet, self).__init__()\n","        self.fc1 = nn.Linear(1024*1*1, 1024)\n","        self.fc2 = nn.Linear(1024, 1024)\n","        self.fc3 = nn.Linear(1024, 1*4)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KtRbzSiQNIFi","colab_type":"code","colab":{}},"source":["model = models.vgg16(pretrained=True)\n","model = model.cuda()\n","model.eval()\n","pool_func = nn.AvgPool2d((7,7), stride=1, ceil_mode=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GiTvIzsfIXV3","colab_type":"code","colab":{}},"source":["trainFeatures=[]\n","\n","for i,data in enumerate(trainLoader):\n","  t0,t1,bbox_0 ,bbox2= data\n","  t0 = t0.cuda()\n","  t1 = t1.cuda()\n","  reshaped = t0.permute(0,3,1,2)\n","  reshaped2 = t1.permute(0,3,1,2)\n","  featureT0 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  featureT1 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  concatted = torch.cat((featureT0[0],featureT1[0]),0).reshape(1,1024)\n","  trainFeatures.append([concatted.data.cpu(),bbox_0,bbox2])\n","  del t0,t1,bbox_0,reshaped,reshaped2,featureT0,featureT1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzfNLFTRIDNU","colab_type":"code","colab":{}},"source":["validFeatures=[]\n","\n","for k,datas in enumerate(validLoader):\n","  t0v,t1v,bbox_0v ,bbox2v= datas\n","  t0v = t0v.cuda()\n","  t1v = t1v.cuda()\n","  reshaped = t0v.permute(0,3,1,2)\n","  reshaped2 = t1v.permute(0,3,1,2)\n","  featureT0 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  featureT1 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  concattedv = torch.cat((featureT0[0],featureT1[0]),0).reshape(1,1024)\n","  validFeatures.append([concattedv.data.cpu(),bbox_0v,bbox2v])\n","  del t0v,t1v,bbox_0v,reshaped,reshaped2,featureT0,featureT1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7_d71d9fxBD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"36e78aeb-7b0d-4a60-dc13-9eaddadfac57","executionInfo":{"status":"ok","timestamp":1559322000958,"user_tz":-180,"elapsed":841722,"user":{"displayName":"Baturalp Karamus","photoUrl":"","userId":"18171244626506243987"}}},"source":["print(len(validFeatures[0][0][0]))"],"execution_count":240,"outputs":[{"output_type":"stream","text":["1024\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jv-Fo9LYy0wV","colab_type":"code","colab":{}},"source":["def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n","    best_model_wts = copy.deepcopy(vgg.state_dict())\n","    best_loss = 999999999999999999\n","    avg_loss = 0\n","    avg_acc = 0\n","    avg_loss_val = 0\n","    avg_acc_val = 0\n","    loss_graph_train = []\n","    loss_graph_valid = []\n","    \n","    \n","    for epoch in range(num_epochs):\n","        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n","        print('-' * 10)\n","        \n","        loss_train = 0\n","        loss_val = 0\n","        acc_train = 0\n","        acc_val = 0\n","        \n","        vgg.train(True)\n","        \n","        for i in trainFeatures:\n","                \n","            feature = i[0].float()\n","            label = i[2].float()\n","            \n","            feature = feature.cuda()\n","            label = label.cuda()\n","            optimizer.zero_grad()\n","            \n","            outputs = vgg(feature)\n","            \n","            loss = criterion(outputs, label)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            loss_train += loss.data\n","            \n","            del feature, label, outputs\n","            torch.cuda.empty_cache()\n","            \n","        \n","        print()\n","        avg_loss = loss_train  / len(trainFeatures)\n","        loss_graph_train.append(avg_loss.data.cpu().item())\n","        vgg.train(False)\n","        vgg.eval()\n","            \n","        for i in validFeatures:\n","                \n","            feature = i[0].float()\n","            label = i[2].float()\n","            \n","            feature = feature.cuda()\n","            label = label.cuda()\n","            \n","            optimizer.zero_grad()\n","            \n","            outputs = vgg(feature)\n","            loss = criterion(outputs, label)\n","            loss_val += loss.data\n","            del feature, label, outputs\n","            torch.cuda.empty_cache()\n","        \n","        avg_loss_val = loss_val / len(validFeatures)\n","        \n","        \n","        loss_graph_valid.append(avg_loss_val.data.cpu().item())\n","        \n","        print()\n","        print(\"Epoch {} result: \".format(epoch))\n","        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n","        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n","        print('-' * 10)\n","        print()\n","        \n","        if avg_loss_val < best_loss:\n","            best_loss = avg_acc_val\n","            best_model_wts = copy.deepcopy(vgg.state_dict())\n","    plt.figure(figsize=(8, 6))\n","    epochs = []\n","    for i in range(num_epochs):\n","      epochs.append(i+1)\n","    plot1, = plt.plot(np.array(epochs),np.array(loss_graph_train))\n","    plot2, = plt.plot(np.array(epochs),np.array(loss_graph_valid))\n","    plt.legend([plot1, plot2], [\"train_loss\", \"validation_loss\"])\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average Negative Log Likelihood')\n","    plt.title('Training and Validation Losses')\n","        \n","    print()\n","    print(\"Best loss: {:.4f}\".format(best_loss))\n","    vgg.load_state_dict(best_model_wts)\n","    return vgg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFmoNRU0M5PM","colab_type":"code","colab":{}},"source":["x = ConvNeuralNet()\n","x.cuda()\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.Adam(x.parameters(), lr=0.0001)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 4, gamma = 0.1)\n","train_model(x,criterion,optimizer,exp_lr_scheduler,10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wLkmnvP8W5hj","colab_type":"code","colab":{}},"source":["testFeatures=[]\n","\n","for k,datas in enumerate(testLoader):\n","  t0c,t1c,bbox_0c= datas\n","  t0c = t0c.cuda()\n","  t1c = t1c.cuda()\n","  reshaped = t0c.permute(0,3,1,2)\n","  reshaped2 = t1c.permute(0,3,1,2)\n","  featureT0 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  featureT1 = (pool_func(model.features(reshaped))).reshape(1,512)\n","  concattedc = torch.cat((featureT0[0],featureT1[0]),0).reshape(1,1024)\n","  testFeatures.append([concattedc.data.cpu(),bbox_0c])\n","  del t0c,t1c,bbox_0c,reshaped,reshaped2,featureT0,featureT1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"riTmbnw0hO6p","colab_type":"code","colab":{}},"source":["def eval_model(vgg, criterion):\n","    outputs = []\n","    avg_loss = 0\n","    loss_test = 0\n","    loss_val = 0\n","    print(\"Evaluation model\")\n","    print('-' * 10)\n","    vgg.train(False)\n","    vgg.eval()\n","    for i in testFeatures:\n","        temp= []\n","        feature = i[0].float()\n","        \n","        labels = i[1].float()\n","        \n","        feature = feature.cuda()\n","        labels = labels.cuda()\n","        output = vgg(feature)\n","        \n","        loss = criterion(output,labels)\n","            \n","        loss_val += loss.data\n","        \n","        outputs.append(output.data.cpu().numpy())\n","        \n","        del feature, output,labels\n","        torch.cuda.empty_cache()\n","        \n","    avg_loss_val = loss_val / len(testFeatures)\n","    \n","    print()\n","    print(\"Avg loss (test): {:.4f}\".format(avg_loss_val))\n","    print('-' * 10)\n","    return outputs\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmuMNnSI7dIY","colab_type":"code","colab":{}},"source":["coord = eval_model(x,criterion)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgZnAUA085Sr","colab_type":"code","colab":{}},"source":["for i in range(70,140):\n","  coords = coord[i]\n","  im = testLoader.dataset[i][3]\n","  x1 = coords[0][0] * len(im[0])/224\n","  y1 = coords[0][1] * len(im)/224\n","  x2 = coords[0][2] * len(im[0])/224\n","  y2 = coords[0][3] * len(im)/224\n","  bbox2 = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n","             linewidth=2, facecolor='none',edgecolor = \"white\")\n","  plt.figure()\n","  fig, ax = plt.subplots(1)\n","  ax.add_patch(bbox2)\n","  plt.imshow(im)\n","  plt.text(x1, y1, s=str(x1)+\",\"+str(y1), \n","                color='white', verticalalignment='top',\n","                bbox={'color': \"green\", 'pad': 0})\n","  plt.axis('off')\n","  plt.savefig(\"/content/drive/My Drive/image/{}.jpg\".format(i))\n","  plt.show()\n","  plt.close(fig)"],"execution_count":0,"outputs":[]}]}